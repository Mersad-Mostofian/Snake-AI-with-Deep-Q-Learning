# ğŸ Snake AI with Deep Q-Learning ğŸš€

This project develops a Snake game using **Deep Q-Learning**. An agent ğŸ¤– is trained using artificial neural networks (MLP) to play the game autonomously and achieve higher scores.

## ğŸ“Œ Prerequisites

To run this project, install the following dependencies:

```bash
pip install numpy pygame tensorflow
```

## ğŸ“‚ Project Structure

```
./
â”‚â”€â”€ classes/
â”‚   â”œâ”€â”€ agent.py       # ğŸ§  Reinforcement learning agent implementation
â”‚   â”œâ”€â”€ qTrainer.py    # ğŸ¯ Trainer for updating the Q-Learning model
â”‚   â”œâ”€â”€ snake.py       # ğŸ Snake game implementation
â”‚â”€â”€ models/
â”‚   â”œâ”€â”€ linear_qnet.py # ğŸ—ï¸ Neural network model for Q-value prediction
â”‚â”€â”€ weights/           # ğŸ’¾ Directory for storing trained model weights
â”‚â”€â”€ main.py            # ğŸ® Run the classic Snake game
â”‚â”€â”€ train_main.py      # ğŸ‹ï¸ Run Snake with reinforcement learning
â”‚â”€â”€ README.md          # ğŸ“œ Project documentation
```

## ğŸ® Running the Classic Snake Game

To play the game manually using the WASD keys, run:

```bash
python main.py
```

## ğŸ† Training the Reinforcement Learning Model

To train the model and observe the agent's learning progress, run:

```bash
python train_main.py
```

âš ï¸ **Note:** Training may take a long time â³, as the model is trained for **50,000 episodes**.

---

## ğŸ§  How Reinforcement Learning Works in This Project

### ğŸ“Œ State Representation:
- ğŸ“ Snake head position
- ğŸ Food position relative to the snake
- ğŸš§ Obstacles (walls and snake's body)
- ğŸ§­ Current movement direction

### ğŸ—ï¸ Neural Network Model:
- ğŸ”¹ One hidden layer with **32 neurons** and **ReLU activation**
- ğŸ¯ Output consists of four values representing actions: **left, right, up, and down**

### ğŸ² Action Selection Policy:
- ğŸ² **Exploration**: The agent moves randomly in the early training phase.
- ğŸ¯ **Exploitation**: The agent uses Q-values to make decisions after training.
- ğŸ“‰ The **epsilon** value decreases gradually to encourage optimal decision-making.

### ğŸ‹ï¸ Trainer:
- ğŸ”„ Q-Learning is applied using the **Bellman equation** update.
- ğŸ“ **Mean Squared Error (MSE)** is used as the loss function to minimize Q-value differences.

---

## ğŸš€ Performance Improvements

âœ… Increasing memory size in the agent to store more experiences  
âœ… Implementing **Double Q-Learning** to reduce Q-value estimation errors  
âœ… Adjusting the **gamma** value to emphasize future rewards  
âœ… Evaluating model performance after training  

---


